<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>go on 是不是很酷</title>
    <link>https://www.syst.top/tags/go/</link>
    <description>Recent content in go on 是不是很酷</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-hans</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Thu, 29 Jun 2023 15:20:00 +0800</lastBuildDate><atom:link href="https://www.syst.top/tags/go/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>socks5结合抓包详解</title>
      <link>https://www.syst.top/posts/go/socks5/</link>
      <pubDate>Thu, 29 Jun 2023 15:20:00 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/socks5/</guid>
      <description>SOCKS5（Socket Secure 5）是一种网络协议，用于在客户端和代理服务器之间进行通信。它是SOCKS协议的第五个版本，SOCKS5协议支持TCP和UDP协议，并提供了认证和加密的功能。
SOCKS5协议广泛用于代理服务器、xx上网、匿名访问、负载均衡等场景。它提供了一种通用的、灵活的代理解决方案，可以在各种网络环境和应用中使用。
RFC 1928是关于SOCKS5的协议文档。
协议解析
SOCKS5工作在应用层和传输层之间，首先客户端需要和代理服务器进行tcp连接。
连接完成后，客户端和代理服务器之间进行协商确定认证方式。
具体就是客户端发送认证方法请求，
 VER字段占1个字节，表示SOCKS协议的版本号，目前为5（即SOCKS5）。 NMETHODS字段占1个字节，表示客户端支持的认证方法数量。 METHODS字段占N个字节，表示客户端支持的认证方法列表。每个字节代表一种认证方法，取值范围为1到255，对应不同的认证方式。  代理服务器在收到客户端消息，从客户端提供的METHODS字段中选择一种支持的认证方法，并将该方法的标识填充到METHOD字段中。代理服务器通过这个应答消息告知客户端选择的认证方法。
目前的METHOD包含以下几个值：
 X&#39;00&amp;rsquo;：NO AUTHENTICATION REQUIRED。表示客户端无需进行任何认证，可以直接进行连接或操作。 X&#39;01&amp;rsquo;：GSSAPI。表示使用GSSAPI（Generic Security Services Application Program Interface）进行认证。 X&#39;02&amp;rsquo;：USERNAME/PASSWORD。表示客户端需要使用用户名和密码进行认证。 X&#39;03&amp;rsquo;到X&#39;7F&amp;rsquo;：由IANA（Internet Assigned Numbers Authority）分配的认证方法。这些方法可能具有特定的定义和用途，可以根据具体的分配情况来确定其含义。 X&#39;80&amp;rsquo;到X&amp;rsquo;FE&amp;rsquo;：保留给私有方法（RESERVED FOR PRIVATE METHODS）。这些方法可能由特定实现或组织自定义，不在通用的认证方法范围内。 X&amp;rsquo;FF&amp;rsquo;：无可接受的方法（NO ACCEPTABLE METHODS）。表示代理服务器无法接受客户端提供的任何认证方法，无法进行连接或操作。  如果返回的方法是X &amp;lsquo;FF&amp;rsquo;，意味着失败了，那么客户端需要关闭连接。
一旦协商完毕，客户端会发送请求的详细信息，主要包括实际要请求的目的地址和端口号。
其中各字段含义如下：
 VER：协议版本，固定为X&#39;05&amp;rsquo;。 CMD：连接命令，指定客户端的操作类型。常见的取值有：  X&#39;01&amp;rsquo;：CONNECT。 X&#39;02&amp;rsquo;：BIND。 X&#39;03&amp;rsquo;：UDP。   RSV：保留字段，固定为X&#39;00&amp;rsquo;。 ATYP：目标地址类型，指定DST.ADDR字段的类型。常见的取值有：  X&#39;01&amp;rsquo;：IPv4地址。 X&#39;03&amp;rsquo;：域名。 X&#39;04&amp;rsquo;：IPv6地址。   DST.ADDR：目标地址，根据ATYP字段的类型来确定具体的格式。 DST.PORT：目标端口，表示客户端要连接的目标服务器的端口号。  代理服务器收到请求后，会发起到DST.ADDR:PORT的连接，并响应客户端结果，
其中的值有：
REP:
 X&#39;00&amp;rsquo; 成功 X&#39;01&amp;rsquo; SOCKS服务器故障 X&#39;02&amp;rsquo; 连接不符合规则 X&#39;03&amp;rsquo; 网络不可达 X&#39;04&amp;rsquo; 主机不可达 X&#39;05&amp;rsquo; 连接被拒绝 X&#39;06&amp;rsquo; TTL 过期 X&#39;07&amp;rsquo; 命令不支持 X&#39;08&amp;rsquo; 地址类型不支持 X&#39;09&amp;rsquo; to X&amp;rsquo;FF&amp;rsquo; 未分配  RSV：预留位，必须设置成X&#39;00&amp;rsquo;。</description>
    </item>
    
    <item>
      <title>easyio:最小化的netpoll实现</title>
      <link>https://www.syst.top/posts/go/easyio/</link>
      <pubDate>Sat, 03 Jun 2023 17:06:51 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/easyio/</guid>
      <description>目前Go圈有很多款异步的网络框架:
 https://github.com/tidwall/evio https://github.com/lesismal/nbio https://github.com/panjf2000/gnet https://github.com/cloudwego/netpoll &amp;hellip;&amp;hellip;.  排名不分先后。
这里面最早的实现是evio。evio也存在一些问题，之前也写过evio文章介绍过。 其他比如nbio和gnet也写过一些源码分析。
为什么会出现这些框架？之前也提到过，由于标准库netpoll的一些特性:
 一个conn一个goroutine导致利用率低 用户无法感知conn状态 &amp;hellip;..  这些框架在应用层上做了很多优化，比如:Worker Pool,Buffer,Ring Buffer,NoCopy&amp;hellip;&amp;hellip;。
都分析了好几篇的代码了，那么咋么说也得自己动手搞一个来达成学习目的。
没错，这就是easyio的由来。
它是一个最小化的IO框架，只实现最核心的部分，加起来不超过500行代码。
也没有用户端上层应用的优化，且目前只实现了linux的epoll，以及只能运行tcp协议。
大概结构如下，具体可以看代码～～，
简单的demo，
服务端:
package main import ( &amp;#34;context&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;os&amp;#34; &amp;#34;os/signal&amp;#34; &amp;#34;syscall&amp;#34; &amp;#34;github.com/wuqinqiang/easyio&amp;#34; ) var _ easyio.EventHandler = (*Handler)(nil) type Handler struct{} type EasyioKey struct{} type Message struct{ Msg string } var CtxKey EasyioKey func (h Handler) OnOpen(c easyio.Conn) context.Context { return context.WithValue(context.Background(), CtxKey, Message{Msg: &amp;#34;helloword&amp;#34;}) } func (h Handler) OnRead(ctx context.</description>
    </item>
    
    <item>
      <title>nbio原理解析</title>
      <link>https://www.syst.top/posts/go/nbio/</link>
      <pubDate>Thu, 23 Feb 2023 23:23:51 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/nbio/</guid>
      <description>之前更新的一系列，好久没更新了，差点烂尾，我不允许这样的事情在我身上发生(虽然已经烂尾好几次了😭)。
在上一篇文章中，我们探讨了基于 epoll 的 Go Netpoll 框架的早期实现——evio。我们还指出了它存在的一些问题。在本篇文章中，我们将继续深入分析另一个高性能的网络编程框架：nbio。
nbio项目里也包含了在nbio之上构建的nbhttp，这个不在我们讨论范围。
nbio同样采用了经典的Reactor模式，事实上，Go语言中的许多异步网络框架都是基于这种模式设计的。
老规矩，先运行nbio程序代码，
Server:
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;github.com/lesismal/nbio&amp;#34; ) func main() { g := nbio.NewGopher(nbio.Config{ Network: &amp;#34;tcp&amp;#34;, Addrs: []string{&amp;#34;:8888&amp;#34;}, MaxWriteBufferSize: 6 * 1024 * 1024, }) g.OnData(func(c *nbio.Conn, data []byte) { c.Write(append([]byte{}, data...)) }) err := g.Start() if err != nil { fmt.Printf(&amp;#34;nbio.Start failed: %v\n&amp;#34;, err) return } defer g.Stop() g.Wait() } 使用nbio.NewGopher()函数创建一个新的Engine实例。传入nbio.Config结构体来配置 Engine 实例，包括：
 Network: 使用的网络类型，这里是tcp。 Addrs: 服务器要监听的地址和端口，这里是 &amp;ldquo;:8888&amp;rdquo;（即监听本机的8888端口）。 MaxWriteBufferSize: 写缓冲区的最大大小，这里设置为 6MB。  其他配置可以自行查看。然后使用g.</description>
    </item>
    
    <item>
      <title>HelloWord一个go开发的学习英语单词工具</title>
      <link>https://www.syst.top/posts/go/helloword/</link>
      <pubDate>Tue, 21 Feb 2023 18:23:51 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/helloword/</guid>
      <description>图片拍摄于2023年02月19日 杭州玉鸟集
背景 Hello Word是我在背单词过程中产生的一个想法。
在学习英语时，词汇量是非常重要的。但是仅仅死记硬背单词，没有语境感，效率是很低的。
虽然一些应用程序可以根据单词的多个词义为单词组成一小段句子，稍微增强语境感。但是单词仍然过于零散。
因此，我们是否可以将每天背诵的多个单词组合成一段小短文，以便复习这一批单词呢？这就是Hello Word的初衷。
当然，ChatGPT API暂时是实现这个想法的工具。 除此之外，程序还配套了几个周边小游戏。
单词短语推送器 指定单词数量，随机选择单词，生成一段小短文，推送到用户指定平台。
这个脚本有以下可选项：
 files：默认导入 CET4.txt 单词文件，你可以通过逗号同时导入多个单词文件，它们都存储在 library 文件夹下。 spec：表示推送频率设置，默认为每小时生成一个新的短语，具体时间规则使用的是 robif/cron 库，请参考该库的文档自行设置。 word-number：表示生成一次短语使用的单词数量，默认为 5 个，最多不超过 10 个  效果
单词选择规则，
 默认:随机 最近最少推送(todo)  单词游戏 单词接龙 这是一个单词接龙游戏，游戏开始时系统会随机选择一个单词。玩家需要以该单词的最后一个字母为开头输入一个新单词，接着程序又以玩家输入单词的最后一个字母为开头输出新单词。游戏会持续进行，直到有一方出现错误。
在一局游戏中，每个单词只能被使用一次。
使用
效果
后续规划
 单词正确性校验，是否是合法的英语单词(todo) 超时控制，用户每个回合指定时间内未输出，游戏结束(todo) 错误机会，一局游戏可以错误次数(todo)  其他游戏 单词拼写(todo)、单词填空(todo) 项目地址在: https://github.com/wuqinqiang/helloword
觉得不错可以点个star，感兴趣可以一起开发。</description>
    </item>
    
    <item>
      <title>发现conc并发库一个有趣的问题</title>
      <link>https://www.syst.top/posts/go/conc/</link>
      <pubDate>Tue, 10 Jan 2023 18:23:51 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/conc/</guid>
      <description>上周看到一个新库conc，
 better structured concurrency for go
 这个库的目标是，
 更难出现goroutine泄漏 优雅处理panic 使并发的代码更易读  我们一条条细说。
Make it harder to leak goroutines goroutine泄漏还是很常见的。
日常我们使用go的时候直接go func开启一个goroutine，写上对应的逻辑，g会进入到某个p的本地队列，最终由p绑定的m执行这个goroutine。
你能保证一个goroutine在某个时刻一定会结束它的生命周期吗？
搜了下著名开源项目etcd，goroutine leak还真不少。
看其中一个简单的泄漏bug。
done是一个无缓冲的chan，一开始接收动作在最下面，因为中间还有一些没展开的代码可能会导致程序不会执行到&amp;lt;-done，然后goroutine就会发生泄漏，解决方法就是通过defer保证一定会执行&amp;lt;-done。
那么conc是如何做的？
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;github.com/sourcegraph/conc&amp;#34; ) func main() { var wg conc.WaitGroup wg.Go(func() { fmt.Println(&amp;#34;g1&amp;#34;) }) wg.Go(func() { fmt.Println(&amp;#34;g2&amp;#34;) }) wg.Wait() } ps:这个结构是不是超级熟悉。
conc的理念是程序中的每一个goroutine由一个owner创建，归属于owner。一个owner确保它拥有的所有goroutine正常退出，这里的owner也就是conc.WaitGroup。
但是这真的能像他说的那样Make it harder to leak goroutines吗？
goroutine的泄漏问题取决于用户对goroutine能正确退出的逻辑保证，和你如何封装没关系吧？
在我看来conc和标准库的sync.WaitGroup一样，等待所有goroutine执行完毕，可以检测到这个行为。
要是goroutine里面含有泄漏的bug，该泄漏还得泄漏，Wait该等待还得老实等待。
Handle panics gracefully 如果直接使用go func,那么可能每一个goroutine都得写上recover，所以一般我们在使用goroutine的时候，都是自己封装一个GoSafe函数。这样就可以在里面统一捕获panic，然后打包调用栈一些信息，进一步处理。
conc里面因为每个goroutine有owner概念，所以是由owner捕获goroutine的panic。</description>
    </item>
    
    <item>
      <title>evio原理解析～有彩蛋</title>
      <link>https://www.syst.top/posts/go/evio/</link>
      <pubDate>Fri, 06 Jan 2023 19:23:51 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/evio/</guid>
      <description>之前分析过go自带的netpoll，以及自建的网络框架gnet。
当然这类框架还有:evio、gev、nbio、cloudwego/netpoll(字节的)。
为什么会出现这么多自建框架?
我觉得逃不过三点，
  自带的netpoll满足不了一些特殊场景。
  其他实现设计存在局限性，存在优化空间。
  程序员都喜欢自己造轮子。
  另外，这类框架都是基于syscall epoll实现的事件驱动框架。主要区别我觉得在于，
 对连接conn的管理 对读写数据管理  带着这些问题，我打算把这些框架都看一遍。学习里面优秀的设计以及对比他们的不同点，可以的话，做个整体的性能测试。
这几个框架中，evio是最早的开源实现，开源于2017年。
有意思的是，看到几篇文章说evio存在当loopWrite在内核缓冲区满，无法一次写入时，会出现写入数据丢失的bug。
仔细阅读了代码，evio并不存在这个bug。也不存在是作者后来修复了这个bug，而是evio本身不存在这个bug。
下面会说明。
原理解析 根据代码画了个简易架构图说明evio架构。
简单解释一下，evio启动的时候可以指定loops个数，即多少个epoll实例。同时可以启动多个监听地址，比如图中监听了两个端口。
程序会把每个Listener fd加入到每个epoll并注册这些fd的读事件。每个epoll会开启一个goroutine等待事件到来。
当客户端发起对应端口连接，程序会根据策略选择一个epoll，并把conn fd 也加入到此epoll并注册读写事件。
当一个conn fd读事件ready，那么对应的epoll会被唤醒，然后执行相应的操作。
以上就是整理的流程，接下来我们来深入一些细节。
在此之前，根据上面所描述的，需要先提几个问题，
 当一个新的客户端连接到来时，会发生什么？ 读写数据是如何流动的？ 同一个epoll里多个fd读写事件ready，程序是如何处理的？  看完下面，再回来回答这三个问题。
代码细节 运行一个简单demo，
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;github.com/tidwall/evio&amp;#34; &amp;#34;log&amp;#34; ) func main() { var events evio.Events events.NumLoops = 3 events.Serving = func(srv evio.Server) (action evio.Action) { log.</description>
    </item>
    
    <item>
      <title>easycar更新日记</title>
      <link>https://www.syst.top/posts/go/easycar2/</link>
      <pubDate>Sun, 06 Nov 2022 10:11:22 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/easycar2/</guid>
      <description>开篇 又拖更了一个多月，在思考人生的意义。
上一次介绍了新开发的一个分布式的事务框架easycar，这篇就当是对easycar的更新日记了。
服务注册与发现
由于easycar底层基于gRPC, 通过自定义Resolver接口还是很容易实现的。
负载均衡
客户端负载均衡，目前支持
  round-robin
  random
  power of 2 random choice
  consistent hash
  ip-hash
  least-load
  架构图
上次提到，参与的一组分布式事务可能部分操作存在先后顺序的问题。
我举了个例子，我们需要保证必须先执行account扣减余额和stock扣减库存服务成功后，才能创建订单order的服务。同时account和stock服务并不需要保证他们的执行顺序。下图，
以这个例子，那么实际在easycar中整个流程，
成功
失败
在easycar中，client负责和easycar(TC)端交互。主要负责注册分支，触发执行分布式事务以及查看状态等。
它并不会和RM产生联系。也就是说它不会负责去请求RM的第一阶段，这是和其他平台不同的一点。
TC全程接管和更新RM状态。
分支状态
项目地址:
Easycar: https://github.com/wuqinqiang/easycar
Client-go: https://github.com/easycar/client-go
Examples： https://github.com/easycar/examples</description>
    </item>
    
    <item>
      <title>一个用go实现的分布式事务框架</title>
      <link>https://www.syst.top/posts/go/easycar/</link>
      <pubDate>Mon, 12 Sep 2022 18:11:22 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/easycar/</guid>
      <description>开篇 对分布式事务一直感兴趣，之前一直被其他事情(懒)耽搁了，最近终于动手了。
easycar是什么 easycar 是一个用go实现的支持两阶段提交协议的分布式事务框架。目前还只支持TCC,SAGA 模式，其他模式待开发。
在介绍easycar 之前，先简单介绍几个角色。
Transaction Coordinator(TC) 负责全局事务的管理，所有参与分布式事务的分支都会注册到coordinator，回给每个分布式事务分配一个唯一id，
当然还包括驱动全局 begin / commit /abort(我喜欢称rollback)。
Transaction Manager (TM) 有些时候也叫 Transaction Client，当然不同的实现也许都会换个名字，但是职责都大差不差。
一般通过TM对每个参与的RM发起一阶段的请求，如果一阶段的RM全部成功，那么TM会向TC发起commit请求，否则发起rollback。
Resource Manager(RM) 用户维度的角色，管理本地事务处理的资源。其实你可以这么理解，假设你的订单服务部分接口参与了分布式事务，无论是第一阶段TM调用接口，还是TC第二阶段调用接口，你的订单服务都会去负责本地的事务修改。
那么 easycar 上述角色有什么不同吗？
有的。既然TC负责的就是全局事务的管理，那么我把职责都给了它。即由TC像每个参与的RM发起一阶段的请求，然后再根据一阶段的结果，发起二阶段的请求。由TC接管整个分布式事务的生命周期。
是的，我弱化了上面TM的能力。在我眼里，TM本质上就是一个客户端。客户端只需要做一些数据封装，简便化操作即可。所以即使没有客户端，其他语言的用户也可以直接通过http请求easycar服务接口。
所以理论上，大部分模式下，不需要客户端也是可以直接使用easycar服务的。
支持协议和事务模式同时混用 参与分布式事务的服务往往由不同的多个部门维护，或者部分新老项目交错，可能无法保证服务的协议是一致的。
另外，不同的服务所采用的事务模式具体是由：业务场景以及构造的成本来决定的。所以参与分布式事务之间所使用的事务模式不一定是统一的。
在这些基础上，easycar支持协议混用(目前支持http和原生的grpc服务)，支持部分事务模式混用(目前支持TCC,Saga)。
支持并发执行 假如现在有 order，account以及stock三个服务。
由这三个服务组成一个分布式事务。 当用户下单时，需要经过这三个服务中内部一些接口(account 扣钱，stock减库存，order 创建订单)。
如果只是同步执行第一阶段，那么第一阶段总执行时间= (account+stock+order)。
很多场景下，分布式事务之间并不会存在执行依赖先后的关系。所以多个子事务一阶段可以同时并发执行。
流程就像这样：
上图我们需要保证创建订单前必须先执行account扣减余额和stock扣减库存服务，才能创建订单order的服务。同时account和stock服务并不需要保证他们的执行顺序。
那么我们一阶段总执行耗时可以粗略=max(account,stock)+order。
因此，easycar是支持分层并发执行的。 对参与的RM通过设置的权重做分层，同一层的RM可以并发调用，一层处理完毕再接下一层。在这个基础上，当某个RM发生调用错误时，那么后面一层也不会执行了，整个分布式事务需要回滚。
异常处理 分布式事务中会出现一些问题，比如
  空补偿： Cancel请求到来时，Try还没有执行，这时候这样的请求我们不能执行，理应直接返回。
  悬挂： Try执行时，Cancel已执行完成，不能执行，直接返回。
  幂等： 所有操作的接口都存在这个问题。
  这些问题需要用户自己去解决，框架不会自动帮你处理。
在我看来，这些问题本身就是服务的必要工作，而不是通过外部服务来帮你保证。
换句话说，前端说它参数做了校验，难道后端就不校验接口了吗？</description>
    </item>
    
    <item>
      <title>Gnet原理解析</title>
      <link>https://www.syst.top/posts/go/gnet/</link>
      <pubDate>Tue, 07 Jun 2022 16:23:51 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/gnet/</guid>
      <description>距离上次写文章过了一月有余，这段时间着实太躺了。以至于昨晚做了一个噩梦，醒来的时候狠狠的抽了自己两巴掌，不能这么躺了。
上面当然是个笑话。
开篇 上一篇我们分析了Go原生网络模型以及部分源码，绝大部分场景下(99%)，使用原生netpoll已经足够了。
但是在一些海量并发连接下，原生netpoll会为每一个连接都开启一个goroutine处理，也就是1千万的连接就会创建一千万个goroutine。这就给了这些特殊场景下的优化空间，这也是像gnet和cloudwego/netpoll诞生的原因之一吧。
本质上他们的底层核心都是一样的，都是基于epoll(linux)实现的。
只是对事件发生后，每个库的处理方式会有所不同。
本篇文章主要分析gnet的。至于使用姿势就不发了，gnet有对应的demo库，可以自行体验。
架构 直接引用gnet官网的一张图
gnet采用的是『主从多 Reactors』。也就是一个主线程负责监听端口连接，当一个客户端连接到来时，就把这个连接根据负载均衡算法分配给其中一个sub线程，由对应的sub线程去处理这个连接的读写事件以及管理它的死亡。
下面这张图就更清晰了。
核心结构 我们先解释gnet的一些核心结构。
engine就是程序最上层的结构了。
  ln对应的listener就是服务启动后对应监听端口的监听器。
  lb对应的loadBalancer就是负载均衡器。也就是当客户端连接服务时，负载均衡器会选择一个sub线程，把连接交给此线程处理。
  mainLoop 就是我们的主线程了，对应的结构eventloop。当然我们的sub线程结构也是eventloop。结构相同，不同的是职责。主线程负责的是监听端口发生的客户端连接事件，然后再由负载均衡器把连接分配给一个sub线程。而sub线程负责的是绑定分配给他的连接(不止一个)，且等待自己管理的所有连接后续读写事件，并进行处理。
  接着看eventloop。
 netpoll.Poller:每一个 eventloop都对应一个epoll或者kqueue。 buffer用来作为读消息的缓冲区。 connCoun记录当前eventloop存储的tcp连接数。 udpSockets和connetcions分别管理着这个eventloop下所有的udp socket和tcp连接，注意他们的结构map。这里的int类型存储的就是fd。  对应conn结构，
这里面有几个字段介绍下，
 buffer:存储当前conn对端(client)发送的最新数据，比如发送了三次，那个此时buffer存储的是第三次的数据,代码里有。 inboundBuffer:存储对端发送的且未被用户读取的剩余数据，还是个Ring Buffer。 outboundBuffer:存储还未发送给对端的数据。(比如服务端响应客户端的数据，由于conn fd是不阻塞的，调用write返回不可写的时候，就可以先把数据放到这里)  conn相当于每个连接都会有自己独立的缓存空间。这样做是为了减少集中式管理内存带来的锁问题。使用Ring buffer是为了增加空间的复用性。
整体结构就这些。
核心逻辑 当程序启动时，
会根据用户设置的options明确eventloop循环的数量，也就是有多少个sub线程。再进一步说，在linux环境就是会创建多少个epoll对象。
那么整个程序的epoll对象就是count(sub)+1(main Listener)。
上图就是我说的，会根据设置的数量创建对应的eventloop,把对应的eventloop 注册到负载均衡器中。
当新连接到来时，就可以根据一定的算法(gnet提供了轮询、最少连接以及hash)挑选其中一个eventloop把连接分配给它。
我们先来看主线程，(由于我使用的是mac,所以后面关于IO多路复用，实现部分就是kqueue代码了，当然原理是一样的)
Polling就是等待网络事件到来，传递了一个闭包参数，更确切的说是一个事件到来时的回调函数，从名字可以看出，就是处理新连接的。
至于Polling函数，
逻辑很简单，一个for循环等待事件到来，然后处理事件。
主线程的事件分两种，
一种是正常的fd发生网络连接事件，
一种是通过NOTE_TRIGGER立即激活的事件。
通过NOTE_TRIGGER触发告诉你队列里有task任务，去执行task任务。
如果是正常的网络事件到来，就处理闭包函数，主线程处理的就是上面的accept连接函数。
accept连接逻辑很简单，拿到连接的fd。设置fd非阻塞模式(想想连接是阻塞的会咋么样?),然后根据负载均衡算法选择一个sub 线程，通过register函数把此连接分配给它。
register做了两件事，首先需要把当前连接注册到当前sub 线程的epoll or kqueue 对象中,新增read的flag。</description>
    </item>
    
    <item>
      <title>Go netpoll大解析</title>
      <link>https://www.syst.top/posts/go/netpoll/</link>
      <pubDate>Thu, 14 Apr 2022 10:23:51 +0800</pubDate>
      
      <guid>https://www.syst.top/posts/go/netpoll/</guid>
      <description>开篇 之前简单看过一点go原生netpoll，没注意太多细节。最近从头到尾看了一遍，特写篇文章记录下。文章很长，请耐心看完，一定有所收获。
用户空间和内核空间 在linux中，经常能看到两个词语:User space(用户空间)和Kernel space (内核空间)。
简单的说， Kernel space是linux内核运行的空间，User space是用户程序运行的空间。它们之间是相互隔离的。
现代操作系统都是采用虚拟存储器。那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核，保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。空间分配如下图所示：
Kernel space可以调用系统的一切资源。User space 不能直接调用系统资源，在 Linux系统中，所有的系统资源管理都是在内核空间中完成的。比如读写磁盘文件、分配回收内存、从网络接口读写数据等等。应用程序无法直接进行这样的操作，但是用户程序可以通过内核提供的接口来完成这样的任务。比如像下面这样，
应用程序要读取磁盘上的一个文件，它可以向内核发起一个 “系统调用” 告诉内核：”我要读取磁盘上的某某文件”。其实就是通过一个特殊的指令让进程从用户态进入到内核态，在内核空间中，CPU 可以执行任何的指令，当然也包括从磁盘上读取数据。具体过程是先把数据读取到内核空间中，然后再把数据拷贝到用户空间并从内核态切换到用户态。此时应用程序已经从系统调用中返回并且拿到了想要的数据，继续往下执行用户空间执行逻辑。
这样的话，一旦涉及到对I/O的处理，就必然会涉及到在用户态和内核态之间来回切换。
io模型 网上有太多关于I/O模型的文章，看着看着有可能就跑偏了，所以我还是从 &amp;laquo;UNIX 网络编程&amp;raquo; 中总结的5中I/O模型说起吧。
Unix可用的5种I/O模型。
 阻塞I/O 非阻塞I/O I/O复用 信号驱动式I/O(SIGIO) 异步I/O(POSIX的aio_系列函数)  阻塞I/O 阻塞式I/O下，进程调用recvfrom，直到数据到达且被复制到应用程序的缓冲区中或者发生错误才返回，在整个过程进程都是被阻塞的。
非阻塞I/O 从图中可以看出，前三次调用recvfrom中没有数据可返回，因此内核转而立即返回一个EWOULDBLOCK错误。第四次调用recvfrom时已有一个数据报准备好，它被复制到应用程序缓冲区，于是recvfrom成功返回。
当一个应用程序像这样对一个非阻塞描述符循环调用recvfrom时，我们通常称为轮询(polling)，持续轮询内核，以这种方式查看某个操作是否就绪。
I/O多路复用 有了I/O多路复用(I/O multiplexing)，我们就可以调用 select 或者 poll，阻塞在这两个系统调用中的某一个之上，而不是阻塞在真正的I/O系统调用上。
上面这句话难理解是吧，说白了这里指的是，在第一步中，我们只是阻塞在select调用上，直到数据报套接字变为可读，返回可读条件，这里并没有发生I/O事件，所以说这一步，并没有阻塞在真正的I/O系统调用上。
其他两种就不过多介绍了。
还有一点，我们会经常提到同步I/O和异步I/O。
POSIX 把这两种术语定义如下:
 同步I/O操作(synchronous I/O opetation) 导致请求进程被阻塞，直到I/O操作完成。 异步I/O(asynchronous opetation) 不导致请求进程被阻塞。  基于上面的定义，
异步I/O的关键在于第二步的recrfrom是否会阻塞住用户进程，如果不阻塞，那它就是异步I/O。从上面汇总图中可以看出，只有异步I/O满足POSIX中对异步I/O的定义。
Go netpoller Go netpoller 底层就是对I/O多路复用的封装。不同平台对I/O多路复用有不同的实现方式。比如Linux的select、poll和epoll(具体差别不是很明白可以看这篇)。在MacOS则是kqueue,而Windows是基于异步I/O实现的icop&amp;hellip;&amp;hellip;，基于这些背景，Go针对不同的平台调用实现了多版本的netpoller。
下面我们通过一个demo开始讲解。
很简单一个demo，开启一个tcp服务。然后每来一个连接，就启动一个g去处理连接。处理完毕，关闭连接。
而且我们使用的是同步的模式去编写异步的逻辑，一个连接对应一个g处理，极其简单和易于理解。go标准库中的http.server也是这么干的。</description>
    </item>
    
  </channel>
</rss>
